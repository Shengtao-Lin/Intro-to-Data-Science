{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>: Shengtao Lin<br>\n",
    "<b>NetID</b>:sl1377<br>\n",
    "<b>Notes to Grader</b> (optional): <br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:  SL    \n",
    "\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Univariate Linear Regression\n",
    "\n",
    "### Due Date: Sunday November 17, 2019 on or before 11:59 PM\n",
    "\n",
    "In this lab we will work through the process of:\n",
    "1. implementing a univariate linear regression model\n",
    "2. defining, implementing and testing multiple loss functions \n",
    "3. minimizing loss functions using gradient descent\n",
    "4. comparing with python library functions\n",
    "5. Using the model to predict on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "In the first task of the lab, we will model linear regression based on a data set that contains grades from CS 205 course in fall 2018. The dataset (with no ID's) contain midterm and final exam grades of Rutgers students (including other assignment grades). \n",
    "\n",
    "# Task 1 - Initialization\n",
    "Read the file into a dataframe and keep only the midtermRaw and FinaRaw columns. We will be doing univariate regression on x=midterm, y=finalExam\n",
    "The goal is to find a model that will allow us to predict the final exam score given the midterm score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1  Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midtermRaw</th>\n",
       "      <th>finalRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   midtermRaw  finalRaw\n",
       "3        45.5      62.0\n",
       "4        58.0      60.5\n",
       "5        68.0      32.0\n",
       "6        64.5      50.5\n",
       "7        74.0      51.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CS205_grades_12_19_18_Final.csv\")\n",
    "df_cleaned = df[['midtermRaw','finalRaw']]\n",
    "# drop all undefined rows \n",
    "df_cleaned = df_cleaned.dropna() \n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2 Normalize Data\n",
    "In this task, you need to normalize data using MinMaxScaler from sklearn.preprocessing. Normalize midterm and final scores to be between 0 and 1. X_scaled_values and Y_scaled_values are the normalized midterm and final exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(df_cleaned)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['midtermScaled', 'finalScaled'])\n",
    "\n",
    "X_scaled_values=np.asarray(scaled_df['midtermScaled']).reshape(-1, 1)\n",
    "Y_scaled_values=np.asarray(scaled_df['finalScaled']).reshape(-1, 1)\n",
    "# END SOLUTION\n",
    "\n",
    "\n",
    "# call the scaled vectors x and y\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.3 Plot the data to see if a linear regression line is a good fit\n",
    "It is helpful to understand if the data lends to a linear regression model. In this activity, we will plot the points to see if a line fit to data is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "ax=plt.scatter(x,y)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "#ax.set_xlabel('midtermScaled')\n",
    "#ax.set_ylabel('finalScaled')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4\n",
    "### BEGIN ANSWER\n",
    "question: Based on what you see in the plot, do you think it is fine to use linear regression? Why?\n",
    "\n",
    "I believe there is a linear regression relation between midterm and final grade since we ca see there is a correlation between them and when midterm grade increases, final grade increases\n",
    "### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Manual Exploration of Linear Regression Line\n",
    "In this task we will manually explore the linear regression line. This will give us a good intution about the process.\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "to all data points (x,y), such that the L2 error (or any other error such as L1 or Huber)\n",
    "$$\n",
    "E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. In this task we will manually change the values of theta0 and theta1 such that we obtain the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function h(theta)\n",
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \n",
    "    \"\"\"\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 - Define the square loss (L2) function\n",
    "Define the function, sqerror that computes the error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average square loss or L2 loss in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02348942348323907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def sqerror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model\n",
    "    Input: x, y vectors\n",
    "    Returns: L2 square error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    ht=0\n",
    "    ht=h(theta0, theta1, x)\n",
    "    a= [(ht1-y1)**2 for ht1,y1 in zip(ht,y)]\n",
    "    L2=sum(a)/len(a)\n",
    "    \n",
    "    return list(L2)[0]\n",
    "    \n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "sqerror(x, y, 0.29,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 - Define the L1 Absolute error function\n",
    "Define the function, abserror that computes the avarega absolute error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average abssolute error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12588780834503435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def abserror(x, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: L1 error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    ht=0\n",
    "    ht=h(theta0, theta1, x)\n",
    "    a= [abs(ht1-y1) for ht1,y1 in zip(ht,y)]\n",
    "    L1=sum(a)/len(a)\n",
    "    \n",
    "    return list(L1)[0]\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "abserror(x, y, 0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3 - Define the Pseudo Huber loss function\n",
    "Define the function, huberror that computes the pseudo huber error based on the two arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "The following function finds the average huber error. In this equation, a  = |h(theta) - y| (see notes for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006994227909507368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def huberror(x, y, theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0, theta1 and delta of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: psuedo huber error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    ht=0\n",
    "    ht=h(theta0, theta1, x)  \n",
    "    a= [abs(ht1-y1) for ht1,y1 in zip(ht,y)]\n",
    "    Ldvalue=[delta**2*(math.sqrt(1+(a1/delta)**2)-1) for a1 in a]\n",
    "    Ld=sum(Ldvalue)/len(Ldvalue)\n",
    "    \n",
    "    return Ld\n",
    "    \n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "huberror(x, y, 0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.4 Interactive Exploration.\n",
    "Let us initialize the interat widget (as in Lab5) to create sliders that allows us to change the values of theta0 and theta1 and see how things change. Complete the function f below. The function is expected to get two values theta0 and theta1 and plot both the observed points (x,y) and the regression line on the same plot. It also needs to compute the error and display and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bd2f8b6a4f40728d99d37fc80cb5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=0.0, descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interactive panel\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    # plot the line for theta0 and theta1\n",
    "    y1 = h(theta0, theta1, x) \n",
    "    # compose plot\n",
    "    pylab.plot(x,y1) \n",
    "    \n",
    "    # compute the L2 error for theta0 and theta1 for 5 decimal places\n",
    "    sqerr = round(sqerror(x, y, theta0, theta1),6)\n",
    "    # compute the absolute or L1 error for theta0 and theta1\n",
    "    abserr = round(abserror(x, y, theta0, theta1),4)\n",
    "    # compute the phub error for theta0 and theta1\n",
    "    huberr = round(huberror(x, y, theta0, theta1, 0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huberr))\n",
    "    \n",
    "    # plot the points\n",
    "    pylab.scatter(x, y, alpha=0.5)\n",
    "    pylab.show() # show the plot  \n",
    "\n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.5 Record the best values for each error function\n",
    "Write the \"best\" values you found for theta0 (y-intercept) and theta1 (slope) and the error. \n",
    "This error is the minimum you have observed based on the manual exploration using the widget \n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BEST VALUES FOR AVE SQUARE ERROR\n",
    "theta0 = 0.3\n",
    "theta1 =0.5\n",
    "error = 0.126\n",
    "# BEST VALUES FOR AVE ABS ERROR\n",
    "theta0 =0.3\n",
    "theta1 =0.5\n",
    "error = 0.02353\n",
    "\n",
    "# BEST VALUES FOR AVE HUBER ERROR\n",
    "theta0 =0.2\n",
    "theta1 =0.7\n",
    "error = 0.0012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Gradient Descent\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the error function. The iterative formulas using L2 loss function for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0)-y_j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0 - y_j)*x_j\n",
    "$$\n",
    "\n",
    "The alpha is called the \"learning rate\". It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or be at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1 Compute using Gradient Descent algorithm (L2 loss)\n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compare with the sample output depends on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0:0.05437313432835818 theta1:0.9967095948827293 L2:0.0357338216876744\n",
      "theta0:0.044009367164179046 theta1:0.9593776206883948 L2:0.034276709798248844\n",
      "theta0:0.07069614881245201 theta1:0.9441697657455459 L2:0.03310265351529898\n",
      "theta0:0.07445850260114387 theta1:0.917853040028166 L2:0.032094783720785586\n",
      "theta0:0.09072534967175905 theta1:0.8999270692939488 L2:0.03120557208836749\n",
      "theta0:0.09865378179349293 theta1:0.8787952165048333 L2:0.03041230896784455\n",
      "theta0:0.11058707857120836 theta1:0.8612063155101113 L2:0.02970154508098296\n",
      "theta0:0.11929279485989783 theta1:0.8430938322678296 L2:0.029063615767306053\n",
      "theta0:0.12908170670422459 theta1:0.8267745889439873 L2:0.02849067900847348\n",
      "theta0:0.13745408438385084 theta1:0.8108047769006026 L2:0.02797598156066322\n",
      "theta0:0.1459247851577802 theta1:0.7959626266385375 L2:0.027513557511734624\n",
      "theta0:0.15363876380038088 theta1:0.7817196378809479 L2:0.027098081947230377\n",
      "theta0:0.16113673439377352 theta1:0.7683216378319104 L2:0.026724782782411194\n",
      "theta0:0.1681342991892166 theta1:0.7555611908467732 L2:0.02638937661044948\n",
      "theta0:0.17483191027358902 theta1:0.7435014484390304 L2:0.026088016303452227\n",
      "theta0:0.1811423520316239 theta1:0.7320490552541277 L2:0.025817245755416194\n",
      "theta0:0.1871464583906476 theta1:0.7212058870404141 L2:0.025573959853212473\n",
      "theta0:0.19282441643519968 theta1:0.7109204240254063 L2:0.02535536873133628\n",
      "theta0:0.19821432250772275 theta1:0.7011752660068699 L2:0.025158965731725953\n",
      "theta0:0.2033187419811097 theta1:0.6919353793312275 L2:0.02498249864252007\n",
      "theta0:0.20815989172807386 theta1:0.6831784908992979 L2:0.024823943865514002\n",
      "theta0:0.21274716165545826 theta1:0.6748770388764096 L2:0.02468148320898241\n",
      "theta0:0.21709633483906016 theta1:0.6670087015151075 L2:0.024553483037272197\n",
      "theta0:0.2212183126290394 theta1:0.6595500767728106 L2:0.024438475537209584\n",
      "theta0:0.2251258224445794 theta1:0.6524803052964804 L2:0.024335141886198328\n",
      "theta0:0.22882951641809623 theta1:0.6457788342997017 L2:0.024242297128889496\n",
      "theta0:0.23234032442309024 theta1:0.6394266422973868 L2:0.02415887658896279\n",
      "theta0:0.23566811557544864 theta1:0.6334054275423742 L2:0.02408392366018743\n",
      "theta0:0.23882253441591944 theta1:0.6276980021134428 L2:0.024016578836754773\n",
      "theta0:0.24181255203684543 theta1:0.6222879793208663 L2:0.023956069857088678\n",
      "theta0:0.24464677282199976 theta1:0.6171598820883492 L2:0.02390170284810991\n",
      "theta0:0.24733329333978296 theta1:0.6122990070040363 L2:0.02385285436840317\n",
      "theta0:0.2498798233005805 theta1:0.6076914362304113 L2:0.023808964259042997\n",
      "theta0:0.2522936499832269 theta1:0.6033239657651831 L2:0.023769529220097875\n",
      "theta0:0.2545816931190818 theta1:0.5991840864103972 L2:0.023734097039151542\n",
      "theta0:0.2567505046578008 theta1:0.5952599368509729 L2:0.023702261405659158\n",
      "theta0:0.25880629936708066 theta1:0.5915402762130091 L2:0.02367365725167285\n",
      "theta0:0.26075496565926076 theta1:0.588014448005086 L2:0.023647956565507163\n",
      "theta0:0.2626020865754173 theta1:0.5846723518642843 L2:0.023624864630339186\n",
      "theta0:0.26435295335236314 theta1:0.5815044132795762 L2:0.02360411664460977\n",
      "theta0:0.26601258201231054 theta1:0.5785015569542504 L2:0.023585474685471324\n",
      "theta0:0.2675857268883098 theta1:0.5756551803406743 L2:0.023568724980461378\n",
      "theta0:0.269076894742268 theta1:0.5729571292712162 L2:0.023553675456114857\n",
      "theta0:0.27049035738207483 theta1:0.5703996744361345 L2:0.023540153535405114\n",
      "theta0:0.27183016407255745 theta1:0.5679754893366432 L2:0.023528004158755663\n",
      "theta0:0.27310015303351526 theta1:0.5656776292397361 L2:0.02351708800592954\n",
      "theta0:0.27430396249558153 theta1:0.5634995113164478 L2:0.023507279898405918\n",
      "theta0:0.27544504108736695 theta1:0.561434895763942 L2:0.02349846736392351\n",
      "theta0:0.27652665773593754 theta1:0.559477867941515 L2:0.023490549346730467\n",
      "theta0:0.2775519110191663 theta1:0.5576228214196964 L2:0.023483435048750157\n",
      "theta0:0.2785237380497276 theta1:0.5558644419231653 L2:0.02347704288837483\n",
      "theta0:0.27944492288494027 theta1:0.5541976921042134 L2:0.02347129956494685\n",
      "theta0:0.28031810450497896 theta1:0.5526177971133203 L2:0.023466139218199993\n",
      "theta0:0.28114578437144155 theta1:0.551120230919495 L2:0.023461502673021606\n",
      "theta0:0.2819303335943354 theta1:0.5497007033447406 L2:0.023457336760875396\n",
      "theta0:0.2826739997242349 theta1:0.5483551477734063 L2:0.023453593710103336\n",
      "theta0:0.28337891319129666 theta1:0.5470797095024513 L2:0.023450230598114727\n",
      "theta0:0.2840470934082642 theta1:0.5458707346985189 L2:0.02344720885918126\n",
      "theta0:0.28468045455572166 theta1:0.5447247599306146 L2:0.02344449384219317\n",
      "theta0:0.28528081106571207 theta1:0.5436385022481494 L2:0.02344205441330533\n",
      "theta0:0.2858498828196992 theta1:0.5426088497760713 L2:0.02343986259891712\n",
      "theta0:0.286389300075605 theta1:0.541632852800057 L2:0.023437893264891332\n",
      "theta0:0.28690060813813195 theta1:0.5407077153162754 L2:0.023436123828334075\n",
      "theta0:0.28738527178569256 theta1:0.5398307870214831 L2:0.023434533998630682\n",
      "theta0:0.2878446794666634 theta1:0.5389995557205274 L2:0.02343310554476753\n",
      "theta0:0.2882801472769623 theta1:0.5382116401294897 L2:0.023431822086272088\n",
      "theta0:0.28869292273035907 theta1:0.5374647830538645 L2:0.023430668905373472\n",
      "theta0:0.28908418833230987 theta1:0.5367568449222213 L2:0.023429632778229764\n",
      "theta0:0.28945506496756085 theta1:0.536085797656833 L2:0.023428701823286626\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error threshold, \n",
    "# the function returns theta0 and theta1 when it reaches the error threshold.\n",
    "# The convergence is reached when the abs(newError - oldError) is less than the threshold.\n",
    "\n",
    "# BEGIN SOLUTION  \n",
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    \n",
    "    theta0=0\n",
    "    theta1=1\n",
    "    newError=sqerror(obsX,obsY,theta0,theta1)\n",
    "    oldError=0\n",
    "    iterations=0\n",
    "    while(abs(newError-oldError)>=threshold):\n",
    "        ht=h(theta0, theta1, obsX)\n",
    "        diff=[(ht1-y1) for ht1,y1 in zip(ht,obsY)]\n",
    "        \n",
    "        theta0-=alpha*sum(diff)\n",
    "        theta1-=alpha*(sum([(diff1*x1) for diff1,x1 in zip(diff,obsX)]))\n",
    "        \n",
    "        oldError=newError\n",
    "        newError=sqerror(obsX,obsY,theta0,theta1)\n",
    "        iterations+=1\n",
    "        \n",
    "        print(\"theta0:{} theta1:{} L2:{}\".format(theta0[0],theta1[0],newError))\n",
    "        \n",
    "    return [theta0[0],theta1[0],newError,iterations]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# END SOLUTION\n",
    "\n",
    "[theta0,theta1,newError,iterations] = gd2(x,y,0.01,0.000001)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28945506496756085, 0.536085797656833)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe theta0 and theta1\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 Compute Gradient Descent (Huber)\n",
    "First Compute a formula for Pseudo huber gradient descent using derivative methods discussed in class and recitation. Similar to L2 descent, use the new formulas (obtained from pseudo huber derivatives) to compute values of theta1, theta1, error. The pseudo huber loss function is provided in Activity 2.3. Use that to differentiate the huber function wrt to theta0 and theta1. \n",
    "\n",
    "Please print out the theta0 and theta1 values for each iteration in your function. You may get different output compared with the sample output depending on your initial theta0 and theta1 values. We will accept any answers which are close to the sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0:0.012710052148225607 theta1:0.9975912942623517 L2:0.0014988434939998947\n",
      "theta0:0.020886146665287347 theta1:0.9929456180793372 L2:0.001484003188999996\n",
      "theta0:0.026803172673791207 theta1:0.9872571143357715 L2:0.0014729844589304372\n",
      "theta0:0.03156674965598739 theta1:0.9811135415884872 L2:0.0014637187151256832\n",
      "theta0:0.03572184655656106 theta1:0.9748047894701859 L2:0.0014553781049346324\n",
      "theta0:0.039539573176085904 theta1:0.968473479315646 L2:0.0014475907915224367\n",
      "theta0:0.04315603195268761 theta1:0.9621892419435116 L2:0.0014401797208655103\n",
      "theta0:0.046640326972147955 theta1:0.9559854445098579 L2:0.0014330604694362174\n",
      "theta0:0.05002812022151175 theta1:0.9498774020302114 L2:0.0014261922473273532\n",
      "theta0:0.05333828931395354 theta1:0.9438714430301646 L2:0.0014195488227106913\n",
      "theta0:0.05658123430354084 theta1:0.9379694351901197 L2:0.0014131044876454987\n",
      "theta0:0.05976302746967335 theta1:0.9321710400090977 L2:0.0014068342473914823\n",
      "theta0:0.06288749297726742 theta1:0.9264748332664559 L2:0.0014007179672171281\n",
      "theta0:0.06595726267796714 theta1:0.9208788675937208 L2:0.0013947413747149788\n",
      "theta0:0.06897432217894811 theta1:0.9153809603747243 L2:0.00138889479851633\n",
      "theta0:0.0719402971236639 theta1:0.9099788427199192 L2:0.0013831716660542468\n",
      "theta0:0.07485660439353692 theta1:0.9046702363754923 L2:0.0013775674364459698\n",
      "theta0:0.07772453241923163 theta1:0.8994528928450102 L2:0.0013720789599005248\n",
      "theta0:0.08054528414785185 theta1:0.894324612663394 L2:0.0013667041235062127\n",
      "theta0:0.0833200002731343 theta1:0.8892832542673477 L2:0.0013614416651860427\n",
      "theta0:0.08604977197500593 theta1:0.8843267374383987 L2:0.0013562910708271357\n",
      "theta0:0.08873564802765284 theta1:0.8794530439430354 L2:0.0013512524887854022\n",
      "theta0:0.09137863884527897 theta1:0.8746602167603763 L2:0.001346326609105532\n",
      "theta0:0.0939797188615494 theta1:0.8699463586445255 L2:0.001341514480212552\n",
      "theta0:0.09653982806087505 theta1:0.865309630435181 L2:0.0013368172873706927\n",
      "theta0:0.09905987318977863 theta1:0.8607482493491442 L2:0.0013322361761643926\n",
      "theta0:0.10154072896154105 theta1:0.8562604873647669 L2:0.0013277722066961086\n",
      "theta0:0.10398323931310384 theta1:0.8518446697103779 L2:0.0013234264212110929\n",
      "theta0:0.10638821853417936 theta1:0.8474991734005265 L2:0.0013191998590473832\n",
      "theta0:0.1087564520482794 theta1:0.8432224257689527 L2:0.0013150933069482108\n",
      "theta0:0.11108869688866477 theta1:0.839012903022708 L2:0.0013111067293820888\n",
      "theta0:0.11338568232747684 theta1:0.8348691289179938 L2:0.0013072386143384014\n",
      "theta0:0.11564811131077603 theta1:0.8307896736424893 L2:0.0013034856429882161\n",
      "theta0:0.11787666302099425 theta1:0.8267731528532967 L2:0.001299842922009388\n",
      "theta0:0.12007199619721008 theta1:0.8228182266634002 L2:0.0012963046403372972\n",
      "theta0:0.12223475238434069 theta1:0.8189235983452487 L2:0.0012928648034590993\n",
      "theta0:0.1243655584086255 theta1:0.815088012657353 L2:0.0012895177859151743\n",
      "theta0:0.12646502785042585 theta1:0.811310253875991 L2:0.001286258639446957\n",
      "theta0:0.12853376168312539 theta1:0.8075891437080202 L2:0.001283083219632552\n",
      "theta0:0.13057234839053722 theta1:0.8039235392538727 L2:0.001279988219883418\n",
      "theta0:0.13258136382800514 theta1:0.8003123311308955 L2:0.001276971176786977\n",
      "theta0:0.13456137097634252 theta1:0.7967544418057828 L2:0.0012740304767869868\n",
      "theta0:0.13651291962960801 theta1:0.7932488241442021 L2:0.0012711653650923766\n",
      "theta0:0.1384365459823319 theta1:0.7897944601680712 L2:0.0012683759290102131\n",
      "theta0:0.1403327720426809 theta1:0.786390360010086 L2:0.0012656629914313692\n",
      "theta0:0.14220210480888135 theta1:0.7830355610638945 L2:0.0012630278141111564\n",
      "theta0:0.144045035248657 theta1:0.7797291273392538 L2:0.0012604715264235321\n",
      "theta0:0.14586203735724493 theta1:0.7764701490315622 L2:0.00125799434755999\n",
      "theta0:0.14765356785913406 theta1:0.7732577422811603 L2:0.001255594931622111\n",
      "theta0:0.14942006714341008 theta1:0.7700910490119245 L2:0.0012532702489221588\n",
      "theta0:0.15116196148777686 theta1:0.7669692366373324 L2:0.00125101608645426\n",
      "theta0:0.1528796658511513 theta1:0.7638914974106441 L2:0.0012488278180386875\n",
      "theta0:0.15457358625668827 theta1:0.7608570473335035 L2:0.0012467010348551793\n",
      "theta0:0.15624412122484058 theta1:0.7578651247293648 L2:0.0012446318782419433\n",
      "theta0:0.15789166230410212 theta1:0.7549149886917333 L2:0.001242617131706995\n",
      "theta0:0.15951659403927015 theta1:0.7520059175989238 L2:0.0012406541842573855\n",
      "theta0:0.1611192937140665 theta1:0.7491372078090316 L2:0.0012387409463511822\n",
      "theta0:0.16270013108512224 theta1:0.7463081725738782 L2:0.001236875759314687\n",
      "theta0:0.1642594682093795 theta1:0.7435181411633317 L2:0.0012350573138291145\n",
      "theta0:0.16579765939339963 theta1:0.7407664581697877 L2:0.0012332845813962411\n",
      "theta0:0.16731505125574442 theta1:0.7380524829570447 L2:0.0012315567580468486\n",
      "theta0:0.16881198287815102 theta1:0.7353755892198085 L2:0.001229873217576305\n",
      "theta0:0.17028878601631434 theta1:0.7327351646242878 L2:0.0012282334704475258\n",
      "theta0:0.17174578534036716 theta1:0.7301306105041259 L2:0.0012266371239905529\n",
      "theta0:0.17318329867601437 theta1:0.7275613415882687 L2:0.0012250838401537444\n",
      "theta0:0.1746016372193898 theta1:0.725026785738489 L2:0.0012235732892358115\n",
      "theta0:0.17600110570292068 theta1:0.7225263836752861 L2:0.001222105101431476\n",
      "theta0:0.1773820024971202 theta1:0.7200595886735961 L2:0.001220678821365221\n",
      "theta0:0.178744619645096 theta1:0.7176258662159976 L2:0.0012192938722757915\n",
      "theta0:0.18008924284170696 theta1:0.7152246936013638 L2:0.0012179495350098264\n",
      "theta0:0.18141615138420933 theta1:0.7128555595192035 L2:0.0012166449430523506\n",
      "theta0:0.1827256181308019 theta1:0.7105179636101601 L2:0.00121537909059949\n",
      "theta0:0.18401790950371838 theta1:0.7082114160372827 L2:0.0012141508483976375\n",
      "theta0:0.18529328556413557 theta1:0.7059354370893512 L2:0.0012129589825229867\n",
      "theta0:0.18655200017079343 theta1:0.7036895568286812 L2:0.0012118021735258399\n",
      "theta0:0.1877943012181292 theta1:0.7014733147853741 L2:0.0012106790357426288\n",
      "theta0:0.1890204309371175 theta1:0.699286259691439 L2:0.0012095881378310581\n",
      "theta0:0.19023062623476397 theta1:0.6971279492434208 L2:0.0012085280254071178\n",
      "theta0:0.19142511904626044 theta1:0.6949979498812089 L2:0.0012074972455585287\n",
      "theta0:0.19260413667614187 theta1:0.6928958365725744 L2:0.0012064943717992717\n",
      "theta0:0.19376790211005118 theta1:0.6908211925964008 L2:0.0012055180273703812\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error, \n",
    "# the function returns theta0, theta1, error and iterations\n",
    "# that reaches a minimum error threshold\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    \n",
    "    theta0=0\n",
    "    theta1=1\n",
    "\n",
    "    newError=huberror(obsX,obsY,theta0,theta1,delta)\n",
    "    oldError=0\n",
    "    iterations=0\n",
    "    while(abs(newError-oldError)>=threshold):\n",
    "        ht=h(theta0, theta1, obsX)\n",
    "        ddiff=[(ht1-y1)/delta for ht1,y1 in zip(ht,obsY)]\n",
    "        \n",
    "\n",
    "        \n",
    "        theta0-=alpha*sum([(delta**2)*(0.5)*((1+ddiff1**2)**0.5)*2*ddiff1 for ddiff1 in ddiff])\n",
    "        theta1-=alpha*sum([(delta**2)*(0.5)*((1+ddiff1**2)**0.5)*2*ddiff1*obsX1 for ddiff1,obsX1 in zip(ddiff,obsX)])\n",
    "        \n",
    "        oldError=newError\n",
    "        newError=huberror(obsX,obsY,theta0,theta1,delta)\n",
    "        iterations+=1\n",
    "        \n",
    "        print(\"theta0:{} theta1:{} L2:{}\".format(theta0[0],theta1[0],newError))\n",
    "        \n",
    "    return [theta0[0],theta1[0],newError,iterations]\n",
    "\n",
    "## END SOLUTION\n",
    "# testing    \n",
    "[theta0,theta1,newError,iterations] = gdh(x,y,0.01,0.000001,0.01)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2\n",
    "1. Write the values of theta0, theta1, alpha, error that provided the minimum value through gradient descent\n",
    "2. Experiment the new values of theta0, theta1 to see if the interactive widget shows similar things.\n",
    "\n",
    "##### BEGIN ANSWER\n",
    "theta0:0.19376790211005118, theta1:0.6908211925964008, alpha:0.01, error:0.0012055180273703812\n",
    "\n",
    "The final theta0 and theta1 would change but still close to 0.2 and 0.7 unless the initially set too close such as theta0=.5 and theta1=.5 will result in theta0=0.316 and theta1=0.499\n",
    "\n",
    "##### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3 Compare with Library Estimators\n",
    "Now use the sklearn LinearRegression module to automate this process. What coefficients do you get? Are they close to what you received from gradient descent? Find the error from sklearn package. Is that error smaller or bigger than the squared error you received?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29620134]\n",
      "[[0.5238794]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(x,y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023420461270159783"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(x,y,theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4- Predict the Final Exam Score\n",
    "The regression line was obtained using grades from CS 205 course. We can consider them to be training data. Now we trained a model (with theta0 and theta1) so we can predict the grade for your own course based on your midterm grade.\n",
    "We will do few things before we can accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.1 Read the midterm grades\n",
    "The grade file for CS439 midterm is given in data/CS439_grades.csv. Read this data file to a new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    97 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 856.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    96 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_439 = pd.read_csv(\"data/CS439_grades_03_15_19.csv\")\n",
    "df_439.info()\n",
    "mid = df_439[df_439['midterm']<80]\n",
    "mid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.2 Predict your Grade\n",
    "Scale the values in the midterm grades of CS 439 and compute the estimated final exam grade. Note that this is probably not a very good estimator since we are trying to predict final exam just by using a midterm score. However, using more features such as labs and quiz scores can help improve the accuracy. We will do that in a future lab. The output is shown as values scaled back to percentages (100% max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    midterm      final\n",
      "0     58.50  58.624958\n",
      "1     51.00  54.695862\n",
      "2     55.00  56.791380\n",
      "3     39.50  48.671249\n",
      "4     71.50  65.435390\n",
      "5     56.50  57.577199\n",
      "6     50.00  54.171983\n",
      "7     52.50  55.481681\n",
      "8     51.50  54.957802\n",
      "9     43.00  50.504827\n",
      "10    53.50  56.005561\n",
      "11    51.50  54.957802\n",
      "12    64.00  61.506294\n",
      "13    60.00  59.410777\n",
      "14    55.00  56.791380\n",
      "15    51.50  54.957802\n",
      "16    58.50  58.624958\n",
      "17    51.00  54.695862\n",
      "18    58.00  58.363018\n",
      "19    73.50  66.483149\n",
      "20    62.00  60.458536\n",
      "21    58.50  58.624958\n",
      "22    34.50  46.051852\n",
      "23    55.50  57.053320\n",
      "24    48.00  53.124224\n",
      "25    67.00  63.077933\n",
      "26    61.00  59.934656\n",
      "27    70.00  64.649571\n",
      "28    46.00  52.076465\n",
      "29    63.00  60.982415\n",
      "..      ...        ...\n",
      "66    36.50  47.099611\n",
      "67    48.00  53.124224\n",
      "68    42.00  49.980948\n",
      "69    54.00  56.267500\n",
      "70    61.00  59.934656\n",
      "71    37.50  47.623490\n",
      "72    61.00  59.934656\n",
      "73    61.00  59.934656\n",
      "74    55.50  57.053320\n",
      "75    50.00  54.171983\n",
      "76    51.50  54.957802\n",
      "77    73.00  66.221209\n",
      "78    65.50  62.292114\n",
      "79    69.00  64.125692\n",
      "80    56.00  57.315259\n",
      "81    60.00  59.410777\n",
      "82    57.00  57.839139\n",
      "83    52.50  55.481681\n",
      "84    54.75  56.660410\n",
      "85    62.00  60.458536\n",
      "86    54.50  56.529440\n",
      "87    51.50  54.957802\n",
      "88    37.00  47.361551\n",
      "89    46.50  52.338405\n",
      "90    72.00  65.697330\n",
      "91    61.50  60.196596\n",
      "92    59.50  59.148837\n",
      "93    49.50  53.910043\n",
      "94    51.00  54.695862\n",
      "95    67.00  63.077933\n",
      "\n",
      "[96 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "#print(mid.head(100))\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(mid)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['midtermScaled'])\n",
    "\n",
    "\n",
    "#scaled_values=np.asarray(scaled_df['midtermScaled'])\n",
    "#print(scaled_values)\n",
    "#predict=h(theta0,theta1,scaled_df)\n",
    "#theta=gd2(x,y,0.01,0.000001)\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(x,y)\n",
    "theta0=result.intercept_\n",
    "theta1=result.coef_\n",
    "\n",
    "scaled_df['finalScaled']=h(theta0,theta1,scaled_df)\n",
    "#print(scaled_df)\n",
    "#predict_df=scaler.inverse_transform(predict)\n",
    "predict_df=scaler.inverse_transform(scaled_df)\n",
    "predict_df = pd.DataFrame(predict_df, columns=['midterm','final'])\n",
    "print(predict_df)\n",
    "#p=pd.DataFrame(predict_df)\n",
    "#result=pd.concat([mid, p], axis=1, sort=False)\n",
    "#print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-10, 10-highest) : 10\n",
    "* how can we improve his lab? :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_lab6.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; lab6 <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Developed by A.D. Gunawardena @ 2019 for CS 439"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

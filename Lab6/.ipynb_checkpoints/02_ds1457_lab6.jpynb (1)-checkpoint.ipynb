{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>: Danling Sun<br>\n",
    "<b>NetID</b>: ds1457<br>\n",
    "<b>Recitation (01/02)</b>: 02<br>\n",
    "<b>Notes to Grader</b> (optional):<br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:     DS\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Univariate Linear Regression\n",
    "\n",
    "### Due Date: Tuesday April 4, 2019 on or before 11:59 PM\n",
    "\n",
    "In this lab we will work through the process of:\n",
    "1. implementing a univariate linear regression model\n",
    "2. defining, implementing and testing multiple loss functions \n",
    "3. minimizing loss functions using gradient descent\n",
    "4. comparing with python library functions\n",
    "5. Using the model to predict on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "In the first task of the lab, we will model linear regression based on a data set that contains grades from CS 205 course in fall 2018. The dataset (with no ID's) contain midterm and final exam grades of Rutgers students (including other assignment grades). \n",
    "\n",
    "# Task 1 - Initialization\n",
    "Read the file into a dataframe and keep only the midtermRaw and FinaRaw columns. We will be doing univariate regression on x=midterm, y=finalExam\n",
    "The goal is to find a model that will allow us to predict the final exam score given the midterm score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1  Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midtermRaw</th>\n",
       "      <th>finalRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   midtermRaw  finalRaw\n",
       "3        45.5      62.0\n",
       "4        58.0      60.5\n",
       "5        68.0      32.0\n",
       "6        64.5      50.5\n",
       "7        74.0      51.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CS205_grades_12_19_18_Final.csv\")\n",
    "df_cleaned = df[['midtermRaw','finalRaw']]\n",
    "# drop all undefined rows \n",
    "df_cleaned = df_cleaned.dropna() \n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2 Normalize Data\n",
    "In this task, you need to normalize data using MinMaxScaler from sklearn.preprocessing. Normalize midterm and final scores to be between 0 and 1. X_scaled_values and Y_scaled_values are the normalized midterm and final exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37857143]\n",
      " [0.55714286]\n",
      " [0.7       ]\n",
      " [0.65      ]\n",
      " [0.78571429]\n",
      " [0.47857143]\n",
      " [0.32142857]\n",
      " [0.80714286]\n",
      " [0.58571429]\n",
      " [0.06428571]\n",
      " [0.53571429]\n",
      " [0.46428571]\n",
      " [0.42142857]\n",
      " [0.83571429]\n",
      " [0.35      ]\n",
      " [0.        ]\n",
      " [0.65      ]\n",
      " [0.48571429]\n",
      " [0.55      ]\n",
      " [0.49285714]\n",
      " [1.        ]\n",
      " [0.6       ]\n",
      " [0.31428571]\n",
      " [0.61428571]\n",
      " [0.55714286]\n",
      " [0.48571429]\n",
      " [0.04285714]\n",
      " [0.5       ]\n",
      " [0.23571429]\n",
      " [0.73571429]\n",
      " [0.59285714]\n",
      " [0.48571429]\n",
      " [0.15714286]\n",
      " [0.83571429]\n",
      " [0.14285714]\n",
      " [0.35      ]\n",
      " [0.41428571]\n",
      " [0.62142857]\n",
      " [1.        ]\n",
      " [0.70714286]\n",
      " [0.56428571]\n",
      " [0.9       ]\n",
      " [0.73571429]\n",
      " [0.24285714]\n",
      " [0.41428571]\n",
      " [0.65      ]\n",
      " [0.52142857]\n",
      " [0.65714286]\n",
      " [0.38571429]\n",
      " [0.4       ]\n",
      " [0.55      ]\n",
      " [0.52857143]\n",
      " [0.46428571]\n",
      " [0.81428571]\n",
      " [0.6       ]\n",
      " [0.54285714]\n",
      " [0.1       ]\n",
      " [0.50714286]\n",
      " [0.2       ]\n",
      " [0.63571429]\n",
      " [0.25714286]\n",
      " [0.94285714]\n",
      " [0.36428571]\n",
      " [0.53571429]\n",
      " [0.8       ]\n",
      " [0.34285714]\n",
      " [0.65      ]\n",
      " [0.67142857]\n",
      " [0.60714286]\n",
      " [0.31428571]\n",
      " [0.72142857]\n",
      " [0.55714286]\n",
      " [0.78571429]\n",
      " [0.31428571]\n",
      " [0.95714286]\n",
      " [0.17857143]\n",
      " [0.68571429]\n",
      " [0.44285714]\n",
      " [0.69285714]\n",
      " [0.17857143]\n",
      " [0.47142857]\n",
      " [0.52857143]\n",
      " [0.27142857]\n",
      " [0.45      ]\n",
      " [0.52857143]\n",
      " [0.48571429]\n",
      " [0.74285714]\n",
      " [0.25714286]\n",
      " [0.85714286]\n",
      " [0.29285714]\n",
      " [0.58571429]\n",
      " [0.70714286]\n",
      " [0.65714286]\n",
      " [0.27857143]\n",
      " [0.78571429]\n",
      " [0.25      ]\n",
      " [0.3       ]\n",
      " [0.58571429]\n",
      " [0.97142857]\n",
      " [0.02857143]\n",
      " [0.95      ]\n",
      " [0.44285714]\n",
      " [0.34285714]\n",
      " [0.48571429]\n",
      " [0.40714286]\n",
      " [0.7       ]\n",
      " [0.19285714]\n",
      " [0.37857143]\n",
      " [0.82142857]\n",
      " [0.87142857]\n",
      " [0.47857143]\n",
      " [0.04285714]\n",
      " [0.93571429]\n",
      " [0.47857143]\n",
      " [0.74285714]\n",
      " [0.63571429]\n",
      " [0.35714286]\n",
      " [0.82857143]\n",
      " [0.51428571]\n",
      " [0.22142857]\n",
      " [0.79285714]\n",
      " [0.92142857]\n",
      " [0.6       ]] [[0.79104478]\n",
      " [0.76865672]\n",
      " [0.34328358]\n",
      " [0.61940299]\n",
      " [0.62686567]\n",
      " [0.39552239]\n",
      " [0.43283582]\n",
      " [0.82835821]\n",
      " [0.6641791 ]\n",
      " [0.26119403]\n",
      " [0.74626866]\n",
      " [0.60447761]\n",
      " [0.79850746]\n",
      " [0.73134328]\n",
      " [0.36567164]\n",
      " [0.42537313]\n",
      " [0.63432836]\n",
      " [0.70895522]\n",
      " [0.81343284]\n",
      " [0.55223881]\n",
      " [0.76119403]\n",
      " [0.47014925]\n",
      " [0.53731343]\n",
      " [0.60447761]\n",
      " [0.20895522]\n",
      " [0.44776119]\n",
      " [0.17910448]\n",
      " [0.31343284]\n",
      " [0.35074627]\n",
      " [0.98507463]\n",
      " [0.5       ]\n",
      " [0.43283582]\n",
      " [0.5       ]\n",
      " [0.86567164]\n",
      " [0.40298507]\n",
      " [0.61940299]\n",
      " [0.6641791 ]\n",
      " [0.6119403 ]\n",
      " [1.        ]\n",
      " [0.45522388]\n",
      " [0.56716418]\n",
      " [0.65671642]\n",
      " [0.88059701]\n",
      " [0.45522388]\n",
      " [0.60447761]\n",
      " [0.53731343]\n",
      " [0.79850746]\n",
      " [0.44776119]\n",
      " [0.55970149]\n",
      " [0.76119403]\n",
      " [0.75373134]\n",
      " [0.67164179]\n",
      " [0.42537313]\n",
      " [0.7761194 ]\n",
      " [0.71641791]\n",
      " [0.6119403 ]\n",
      " [0.43283582]\n",
      " [0.61940299]\n",
      " [0.32835821]\n",
      " [0.46268657]\n",
      " [0.50746269]\n",
      " [0.47014925]\n",
      " [0.45522388]\n",
      " [0.48507463]\n",
      " [0.60447761]\n",
      " [0.47014925]\n",
      " [0.82089552]\n",
      " [0.88059701]\n",
      " [0.6641791 ]\n",
      " [0.38059701]\n",
      " [0.75373134]\n",
      " [0.50746269]\n",
      " [0.52238806]\n",
      " [0.53731343]\n",
      " [0.81343284]\n",
      " [0.26119403]\n",
      " [0.69402985]\n",
      " [0.51492537]\n",
      " [0.55223881]\n",
      " [0.49253731]\n",
      " [0.34328358]\n",
      " [0.51492537]\n",
      " [0.54477612]\n",
      " [0.34328358]\n",
      " [0.64179104]\n",
      " [0.74626866]\n",
      " [0.73134328]\n",
      " [0.15671642]\n",
      " [0.95522388]\n",
      " [0.53731343]\n",
      " [0.68656716]\n",
      " [0.40298507]\n",
      " [0.39552239]\n",
      " [0.37313433]\n",
      " [0.93283582]\n",
      " [0.31343284]\n",
      " [0.41791045]\n",
      " [0.38059701]\n",
      " [0.78358209]\n",
      " [0.        ]\n",
      " [0.81343284]\n",
      " [0.2761194 ]\n",
      " [0.50746269]\n",
      " [0.1641791 ]\n",
      " [0.49253731]\n",
      " [0.76119403]\n",
      " [0.51492537]\n",
      " [0.64925373]\n",
      " [0.92537313]\n",
      " [0.58955224]\n",
      " [0.82089552]\n",
      " [0.36567164]\n",
      " [0.85074627]\n",
      " [0.67910448]\n",
      " [0.6641791 ]\n",
      " [0.71641791]\n",
      " [0.44029851]\n",
      " [0.89552239]\n",
      " [0.67910448]\n",
      " [0.29850746]\n",
      " [0.76119403]\n",
      " [0.54477612]\n",
      " [0.44776119]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the numeric data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler() \n",
    "\n",
    "# set of observations\n",
    "X = df_cleaned.iloc[:,[0]]\n",
    "\n",
    "# set of categories\n",
    "Y = df_cleaned.iloc[:,[1]]\n",
    "\n",
    "# Normalizing data\n",
    "X_scaled_values = MinMaxScaler().fit_transform(X) \n",
    "X_scaled_values[:,:] = X_scaled_values\n",
    "\n",
    "Y_scaled_values = MinMaxScaler().fit_transform(Y) \n",
    "Y_scaled_values[:,:] = Y_scaled_values\n",
    "\n",
    "\n",
    "# call the scaled vectors x and y, X_scaled_values is a column matrix\n",
    "\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values\n",
    "\n",
    "\n",
    "print(x, y)\n",
    "\n",
    "#X_scaled_values[:,:] = X_scaled_values ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.3 Plot the data to see if a linear regression line is a good fit\n",
    "It is helpful to understand if the data lends to a linear regression model. In this activity, we will plot the points to see if a line fit to data is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a187fe780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "plt.scatter(x,y, alpha=0.5, s=20)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4\n",
    "### BEGIN ANSWER\n",
    "question: Based on what you see in the plot, do you think it is fine to use linear regression? Why?\n",
    "\n",
    "ANS: I think it is fine to use linear regression. As we know, At the center of the regression analysis is the task of fitting a single line through a scatter plot. According to the data shown above, it is possible to find a fitting line since the distrubution of data is relatively concentrated. Overall, the data has a trend.\n",
    "\n",
    "### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Manual Exploration of Linear Regression Line\n",
    "In this task we will manually explore the linear regression line. This will give us a good intution about the process.\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "to all data points (x,y), such that the error\n",
    "$$\n",
    "E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. In this task we will manually change the values of theta0 and theta1 such that we obtain the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function h(theta)\n",
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \n",
    "    \"\"\"\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1 - Define the square loss (L2) function\n",
    "Define the function, sqerror that computes the error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average square loss or L2 loss in this case. Note that we assume global x and y vectors and do not pass them to the sqerror function and other functions that follow. This is a bad practice, but for the sake of simplicity we will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8891990884384056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02348942348323907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def sqerror(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: L2 square error sum\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    sqerror =0\n",
    "    for i in range (len(x)):\n",
    "        sqerror = sqerror + np.square((h(theta0, theta1, x[i][0])-y[i][0]))\n",
    "    \n",
    "    print (sqerror)\n",
    "    return sqerror/len(x)\n",
    "\n",
    "    ## END SOLUTION\n",
    "\n",
    "## testing\n",
    "sqerror(0.29,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2 - Define the L1 Absolute error function\n",
    "Define the function, abserror that computes the avarega absolute error based on the arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. We use the average abssolute error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.484200426439227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12588780834503435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def abserror(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: L1 error sum\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    abserror = 0\n",
    "    \n",
    "    for i in range (len(x)):\n",
    "        abserror = abserror + np.absolute(h(theta0, theta1, x[i][0])- y[i][0])\n",
    "    \n",
    "    print (abserror)\n",
    "    return abserror/len(x)\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "abserror(0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3 - Define the Pseudo Huber loss function\n",
    "Define the function, phuberror that computes the pseudo huber error based on the two arguments provided. The function h(theta) is as defined above. Assume that x and y are the observed vectors. The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "The following function finds the average huber error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602900328694063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.006994227909507368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "def huberror(theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Input: x, y vectors\n",
    "    Returns: psuedo huber error\n",
    "    Assumptions: none\n",
    "    \"\"\"\n",
    "    huberror = 0\n",
    "    \n",
    "    for i in range (len(x)):\n",
    "\n",
    "        # The a in this case is the difference between estimated and observed values of y. \n",
    "        a  = h(theta0, theta1, x[i][0])-y[i][0]\n",
    "    \n",
    "        second_part = np.sqrt(1+(np.square(a/delta)))-1\n",
    "        huberror = huberror + np.square(delta)* second_part\n",
    "        \n",
    "    print (huberror)    \n",
    "    return huberror/len(x)\n",
    "    \n",
    "## END SOLUTION\n",
    "\n",
    "## testing\n",
    "huberror(0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.4 Interactive Exploration.\n",
    "Let us initialize the interat widget (as in Lab5) to create sliders that allows us to change the values of theta0 and theta1 and see how things change. Complete the function f below. The function is expected to get two values theta0 and theta1 and plot both the observed points (x,y) and the regression line on the same plot. It also needs to compute the error and display and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5223717637d493985a2edd310d06505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=0.0, descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interactive panel\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    # plot the line for theta0 and theta1\n",
    "    y = h(theta0, theta1, x) \n",
    "    # compose plot\n",
    "    pylab.plot(x,y) \n",
    "    \n",
    "    # compute the L2 error for theta0 and theta1 for 5 decimal places\n",
    "    sqerr = round(sqerror(theta0, theta1),6)\n",
    "    # compute the absolute or L1 error for theta0 and theta1\n",
    "    abserr = round(abserror(theta0, theta1),4)\n",
    "    # compute the phub error for theta0 and theta1\n",
    "    huberr = round(huberror(theta0, theta1,0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huberr))\n",
    "    \n",
    "    # plot the points\n",
    "    x1 = X_scaled_values   \n",
    "    y1 = Y_scaled_values\n",
    "    pylab.scatter(x1, y1, alpha=0.5)\n",
    "    # show the plot\n",
    "    pylab.show()  \n",
    "\n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.5 Record the best values for each error function\n",
    "Write the \"best\" values you found for theta0 (y-intercept) and theta1 (slope) and the error. \n",
    "This error is the minimum you have observed based on the manual exploration using the widget \n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST VALUES FOR AVE SQUARE ERROR\n",
    "theta0 = 0.30\n",
    "theta1 = 0.50\n",
    "error = 0.02353\n",
    "\n",
    "# BEST VALUES FOR AVE ABS ERROR\n",
    "theta0 = 0.20\n",
    "theta1 = 0.70\n",
    "error = 0.1288\n",
    "\n",
    "# BEST VALUES FOR AVE HUBER ERROR\n",
    "theta0 = 0.20\n",
    "theta1 = 0.70\n",
    "error = 0.0012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Gradient Descent\n",
    "In this task we need to use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the graph. The iterative formulas for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0)-y_j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0 - y_j)*x_j\n",
    "$$\n",
    "\n",
    "The alpha is called the \"learning rate\". It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or at the risk of over shooting the minimum point (large alpha). You may have to experiemnt with few alphas to find something that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1 Compute using Gradient Descent algorithm (L2 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.951636538874247\n",
      "4.647919199312605\n",
      "4.951636538874247\n",
      "4.647919199312605\n",
      "4.567955836932531\n",
      "4.951636538874247\n",
      "4.336348503913628\n",
      "4.567955836932531\n",
      "4.1676031773754865\n",
      "4.336348503913628\n",
      "4.029675599215039\n",
      "4.1676031773754865\n",
      "3.9105103169288493\n",
      "4.029675599215039\n",
      "3.8050974104124875\n",
      "3.9105103169288493\n",
      "3.7109606799989283\n",
      "3.8050974104124875\n",
      "3.626579755467221\n",
      "3.7109606799989283\n",
      "3.550833653460212\n",
      "3.626579755467221\n",
      "3.4828004233378898\n",
      "3.550833653460212\n",
      "3.421681351272452\n",
      "3.4828004233378898\n",
      "3.3667691124599943\n",
      "3.421681351272452\n",
      "3.3174317675282756\n",
      "3.3667691124599943\n",
      "3.2731027704253197\n",
      "3.3174317675282756\n",
      "3.233273516633592\n",
      "3.2731027704253197\n",
      "3.197487178639524\n",
      "3.233273516633592\n",
      "3.1653333517998656\n",
      "3.197487178639524\n",
      "3.1364433079177716\n",
      "3.1653333517998656\n",
      "3.1104857529509227\n",
      "3.1364433079177716\n",
      "3.087163023107214\n",
      "3.1104857529509227\n",
      "3.0662076697600984\n",
      "3.087163023107214\n",
      "3.047379391953153\n",
      "3.0662076697600984\n",
      "3.030462280598268\n",
      "3.047379391953153\n",
      "3.0152623425156286\n",
      "3.030462280598268\n",
      "3.0016052758359124\n",
      "3.0152623425156286\n",
      "2.989334471224276\n",
      "3.0016052758359124\n",
      "2.989334471224276\n",
      "4.951636538874247\n",
      "4.647919199312605\n",
      "4.951636538874247\n",
      "4.647919199312605\n",
      "4.567955836932531\n",
      "4.951636538874247\n",
      "4.336348503913628\n",
      "4.567955836932531\n",
      "4.1676031773754865\n",
      "4.336348503913628\n",
      "4.029675599215039\n",
      "4.1676031773754865\n",
      "3.9105103169288493\n",
      "4.029675599215039\n",
      "3.8050974104124875\n",
      "3.9105103169288493\n",
      "3.7109606799989283\n",
      "3.8050974104124875\n",
      "3.626579755467221\n",
      "3.7109606799989283\n",
      "3.550833653460212\n",
      "3.626579755467221\n",
      "3.4828004233378898\n",
      "3.550833653460212\n",
      "3.421681351272452\n",
      "3.4828004233378898\n",
      "3.3667691124599943\n",
      "3.421681351272452\n",
      "3.3174317675282756\n",
      "3.3667691124599943\n",
      "3.2731027704253197\n",
      "3.3174317675282756\n",
      "3.233273516633592\n",
      "3.2731027704253197\n",
      "3.197487178639524\n",
      "3.233273516633592\n",
      "3.1653333517998656\n",
      "3.197487178639524\n",
      "3.1364433079177716\n",
      "3.1653333517998656\n",
      "3.1104857529509227\n",
      "3.1364433079177716\n",
      "3.087163023107214\n",
      "3.1104857529509227\n",
      "3.0662076697600984\n",
      "3.087163023107214\n",
      "3.047379391953153\n",
      "3.0662076697600984\n",
      "3.030462280598268\n",
      "3.047379391953153\n",
      "3.0152623425156286\n",
      "3.030462280598268\n",
      "3.0016052758359124\n",
      "3.0152623425156286\n",
      "2.989334471224276\n",
      "3.0016052758359124\n",
      "2.989334471224276\n",
      "(0.22636473304255345, 0.6502386667272365, 0.024303532286376228, 27)\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error threshold, \n",
    "# the function returns theta0 and theta1\n",
    "# that reach the error threshold.\n",
    "# The convergence is reached when the newError - oldError is less than the threshold.\n",
    "\n",
    "\n",
    "# BEGIN SOLUTION  \n",
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \n",
    "    Return theta0, theta1, error and iterations from Gradient Descent algorithm\n",
    "    \"\"\"\n",
    "    sumpart_theta0 =0\n",
    "    sumpart_theta1 =0\n",
    "    #Initialize the theta0 be 0.\n",
    "    Initial_theta0=0\n",
    "    #Initialize the theta1 be 1.\n",
    "    Initial_theta1=1\n",
    "    iterations=0\n",
    "\n",
    "    for i in range (len(obsX)):\n",
    "        sumpart_theta0=sumpart_theta0+(h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])\n",
    "        sumpart_theta1=sumpart_theta0+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])*obsX[i][0])\n",
    "    \n",
    "    theta0=Initial_theta0-alpha*(sumpart_theta0)\n",
    "    theta1=Initial_theta1-alpha*(sumpart_theta1)\n",
    "    iterations=iterations+1\n",
    "    \n",
    "    if (np.absolute(sqerror(theta0,theta1)-sqerror(Initial_theta0,Initial_theta1)) < threshold):\n",
    "        return(theta0,theta1,sqerror(theta0,theta1),iterations)\n",
    "    \n",
    "    else:\n",
    "        while(np.absolute(sqerror(theta0,theta1)-sqerror(Initial_theta0,Initial_theta1)) > threshold):\n",
    "            sumpart_theta0=0\n",
    "            sumpart_theta1=0\n",
    "            Initial_theta0=theta0\n",
    "            Initial_theta1=theta1\n",
    "            for i in range (len(obsX)):\n",
    "                sumpart_theta0=sumpart_theta0+(h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])\n",
    "                sumpart_theta1=sumpart_theta1+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])*obsX[i][0])\n",
    "            theta0=Initial_theta0-alpha*(sumpart_theta0)\n",
    "            theta1=Initial_theta1-alpha*(sumpart_theta1)\n",
    "            iterations=iterations+1\n",
    "        return(theta0,theta1,sqerror(theta0,theta1),iterations)\n",
    "\n",
    "    \n",
    "# END SOLUTION\n",
    "[theta0,theta1,newError,iterations] = gd2(x,y,0.01,0.0001)\n",
    "print(gd2(x,y,0.01,0.0001))\n",
    "print (iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22636473304255345, 0.6502386667272365)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe theta0 and theta1\n",
    "theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2 Compute Gradient Descent (Huber)\n",
    "First Compute a formula for huber gradient descent using derivative methods discussed in class. Similar to L2 descent, use the new formulas (obtained from huber derivatives) to compute values of theta1, theta1, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18435774976198704\n",
      "0.18739213673056318\n",
      "0.18435774976198704\n",
      "0.18739213673056318\n",
      "0.18253239224699952\n",
      "0.18435774976198704\n",
      "0.18117708844844377\n",
      "0.18253239224699952\n",
      "0.18003740196045903\n",
      "0.18117708844844377\n",
      "0.17901150690695977\n",
      "0.18003740196045903\n",
      "0.17805366735725972\n",
      "0.17901150690695977\n",
      "0.17714210566645777\n",
      "0.17805366735725972\n",
      "0.17626643774065476\n",
      "0.17714210566645777\n",
      "0.17542164642126445\n",
      "0.17626643774065476\n",
      "0.17460450519341503\n",
      "0.17542164642126445\n",
      "0.17381185198039634\n",
      "0.17460450519341503\n",
      "0.17304061242915234\n",
      "0.17381185198039634\n",
      "0.17228830996770675\n",
      "0.17304061242915234\n",
      "0.17155318908994238\n",
      "0.17228830996770675\n",
      "0.17083406021750858\n",
      "0.17155318908994238\n",
      "0.17013011492467237\n",
      "0.17083406021750858\n",
      "0.16944079468285428\n",
      "0.17013011492467237\n",
      "0.16876571206776456\n",
      "0.16944079468285428\n",
      "0.16810460719126416\n",
      "0.16876571206776456\n",
      "0.16745732481788325\n",
      "0.16810460719126416\n",
      "0.1668238017117377\n",
      "0.16745732481788325\n",
      "0.16620405612060446\n",
      "0.1668238017117377\n",
      "0.16559817291998044\n",
      "0.16620405612060446\n",
      "0.1650062810661439\n",
      "0.16559817291998044\n",
      "0.1644285263465952\n",
      "0.1650062810661439\n",
      "0.1638650496682203\n",
      "0.1644285263465952\n",
      "0.16331598142362136\n",
      "0.1638650496682203\n",
      "0.16278144980896442\n",
      "0.16331598142362136\n",
      "0.16226158266282814\n",
      "0.16278144980896442\n",
      "0.16175647675462992\n",
      "0.16226158266282814\n",
      "0.16126612771399693\n",
      "0.16175647675462992\n",
      "0.16079034956362337\n",
      "0.16126612771399693\n",
      "0.16032873408755058\n",
      "0.16079034956362337\n",
      "0.15988067940715472\n",
      "0.16032873408755058\n",
      "0.15944547076148755\n",
      "0.15988067940715472\n",
      "0.15902237082546922\n",
      "0.15944547076148755\n",
      "0.15861068766756645\n",
      "0.15902237082546922\n",
      "0.1582098126519757\n",
      "0.15861068766756645\n",
      "0.15781923601480388\n",
      "0.1582098126519757\n",
      "0.15743855104566043\n",
      "0.15781923601480388\n",
      "0.15706745474479816\n",
      "0.15743855104566043\n",
      "0.1567057486447994\n",
      "0.15706745474479816\n",
      "0.15635333990636233\n",
      "0.1567057486447994\n",
      "0.15601023926825622\n",
      "0.15635333990636233\n",
      "0.1556765479460584\n",
      "0.15601023926825622\n",
      "0.15535242113567224\n",
      "0.1556765479460584\n",
      "0.15503799775009444\n",
      "0.15535242113567224\n",
      "0.1547333047498788\n",
      "0.15503799775009444\n",
      "0.15443817658951964\n",
      "0.1547333047498788\n",
      "0.15415224061742552\n",
      "0.15443817658951964\n",
      "0.15387497863387398\n",
      "0.15415224061742552\n",
      "0.15360582161875855\n",
      "0.15387497863387398\n",
      "0.15334422728718705\n",
      "0.15360582161875855\n",
      "0.15308972102375903\n",
      "0.15334422728718705\n",
      "0.15284190719996038\n",
      "0.15308972102375903\n",
      "0.15260046466365842\n",
      "0.15284190719996038\n",
      "0.15236513640119542\n",
      "0.15260046466365842\n",
      "0.1521357183957065\n",
      "0.15236513640119542\n",
      "0.1519120496009811\n",
      "0.1521357183957065\n",
      "0.15169400351173765\n",
      "0.1519120496009811\n",
      "0.15148148123976238\n",
      "0.15169400351173765\n",
      "0.15127440576188553\n",
      "0.15148148123976238\n",
      "0.15107271686504567\n",
      "0.15127440576188553\n",
      "0.150876366250838\n",
      "0.15107271686504567\n",
      "0.15068531233891055\n",
      "0.150876366250838\n",
      "0.1504995145760048\n",
      "0.15068531233891055\n",
      "0.15031892747607156\n",
      "0.1504995145760048\n",
      "0.15014349502792218\n",
      "0.15031892747607156\n",
      "0.14997314628992237\n",
      "0.15014349502792218\n",
      "0.14980779280620865\n",
      "0.14997314628992237\n",
      "0.14964732799543912\n",
      "0.14980779280620865\n",
      "0.1494916281437373\n",
      "0.14964732799543912\n",
      "0.1493405543529094\n",
      "0.1494916281437373\n",
      "0.14919395485032735\n",
      "0.1493405543529094\n",
      "0.1490516673436783\n",
      "0.14919395485032735\n",
      "0.14891352139634334\n",
      "0.1490516673436783\n",
      "0.14877934095322015\n",
      "0.14891352139634334\n",
      "0.14864894712507548\n",
      "0.14877934095322015\n",
      "0.14852216120369902\n",
      "0.14864894712507548\n",
      "0.14839880773131042\n",
      "0.14852216120369902\n",
      "0.14827871736655687\n",
      "0.14839880773131042\n",
      "0.14827871736655687\n",
      "81\n",
      "0.19376790211005118 0.6908211925964008\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error, \n",
    "# the function returns theta0, theta1, error and iterations\n",
    "# that reaches a minimum error threshold\n",
    "\n",
    "## BEGIN SOLUTION (Cooperation with WeiBang He)\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    sumpart_theta0 =0\n",
    "    sumpart_theta1 =0\n",
    "    #Initialize the theta0 be 0.\n",
    "    Initial_theta0=0\n",
    "    #Initialize the theta1 be 1.\n",
    "    Initial_theta1=1\n",
    "    iterations=0\n",
    "\n",
    "    for i in range (len(obsX)):\n",
    "        # a =  h(theta0, theta1, x[i][0])-y[i][0]\n",
    "        sumpart_theta0=sumpart_theta0+(delta**2)*(0.5)*((1+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)**(2))**(0.5))*2*((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)\n",
    "        sumpart_theta1=sumpart_theta1+((delta**2)*(0.5)*((1+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)**(2))**(0.5))*2*((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)*obsX[i][0])\n",
    "        \n",
    "\n",
    "    theta0=Initial_theta0-alpha*(sumpart_theta0)\n",
    "    theta1=Initial_theta1-alpha*(sumpart_theta1)\n",
    "    iterations=iterations+1\n",
    "    \n",
    "    if (np.absolute(huberror(theta0,theta1,delta)-huberror(Initial_theta0,Initial_theta1,delta)) < threshold):\n",
    "        return(theta0,theta1,huberror(theta0,theta1),iterations)\n",
    "    \n",
    "    else:\n",
    "        while(np.absolute(huberror(theta0,theta1,delta)-huberror(Initial_theta0,Initial_theta1,delta)) > threshold):\n",
    "            sumpart_theta0=0\n",
    "            sumpart_theta1=0\n",
    "            Initial_theta0=theta0\n",
    "            Initial_theta1=theta1\n",
    "            for i in range (len(obsX)):\n",
    "                sumpart_theta0=sumpart_theta0+(delta**2)*(0.5)*((1+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)**(2))**(0.5))*2*((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)\n",
    "                sumpart_theta1=sumpart_theta1+((delta**2)*(0.5)*((1+((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)**(2))**(0.5))*2*((h(Initial_theta0,Initial_theta1,obsX[i][0])-obsY[i][0])/delta)*obsX[i][0])\n",
    "        \n",
    "            theta0=Initial_theta0-alpha*(sumpart_theta0)\n",
    "            theta1=Initial_theta1-alpha*(sumpart_theta1)\n",
    "            iterations=iterations+1\n",
    "        return(theta0,theta1,huberror(theta0,theta1,delta),iterations)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "## END SOLUTION\n",
    "# testing    \n",
    "[theta0,theta1,newError,iterations] = gdh(x,y,0.01,0.000001,0.01)\n",
    "print(iterations)\n",
    "print(theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2\n",
    "1. Write the values of theta0, theta1, alpha, error that provided the minimum value through gradient descent\n",
    "2. Experiment the new values of theta0, theta1 to see if the interactive widget shows similar things.\n",
    "\n",
    "##### BEGIN ANSWER\n",
    "ANS1:\n",
    "    theta0: 0.19376790211005118\n",
    "    theta1: 0.6908211925964008\n",
    "    alpha: 0.01\n",
    "    error: 0.0012055180273703812\n",
    "\n",
    "    \n",
    "ANS2:\n",
    "\n",
    "    As the value of theata 0 incresing, the value of theta1 is decresing.\n",
    "\n",
    "##### END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3 Compare with Library Estimators\n",
    "Now use the sklearn LinearRegression module to automate this process. What coefficients do you get? Are they close to what you received from gradient descent? Find the error from sklearn package. Is that error smaller or bigger than the squared error you received?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29620134]\n",
      "[[0.5238794]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(x,y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.88071674]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02342046]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4- Predict Your Final Exam Score\n",
    "The regression line was obtained using grades from CS 205 course. We can consider them to be training data. Now we trained a model (with theta0 and theta1) so we can predict the grade for your own course based on your midterm grade.\n",
    "We will do few things before we can accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.1 Read the midterm grades\n",
    "The grade file for CS439 midterm is given in data/CS439_grades.csv. Read this data file to a new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    97 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 856.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    96 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_439 = pd.read_csv(\"data/CS439_grades_03_15_19.csv\")\n",
    "df_439.info()\n",
    "mid = df_439[df_439['midterm']<80]\n",
    "mid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.2 Predict your Grade\n",
    "Scale the values in the midterm grades of CS 439 and compute the estimated final exam grade. Note that this is probably not a very good estimator since we are trying to predict final exam just by using a midterm score. However, using more features such as labs and quiz scores can help improve the accuracy. We will do that in a future lab. The output is shown as values scaled back to percentages (100% max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midterm Score: 61.500000  Predicted Score : 61.858866\n",
      "Midterm Score: 42.300000  Predicted Score : 51.784262\n",
      "Midterm Score: 52.600000  Predicted Score : 57.157384\n",
      "Midterm Score: 12.800000  Predicted Score : 36.336536\n",
      "Midterm Score: 94.900000  Predicted Score : 79.321513\n",
      "Midterm Score: 56.400000  Predicted Score : 59.172305\n",
      "Midterm Score: 39.700000  Predicted Score : 50.440982\n",
      "Midterm Score: 46.200000  Predicted Score : 53.799183\n",
      "Midterm Score: 43.600000  Predicted Score : 52.455902\n",
      "Midterm Score: 21.800000  Predicted Score : 41.038018\n",
      "Midterm Score: 48.700000  Predicted Score : 55.142463\n",
      "Midterm Score: 43.600000  Predicted Score : 52.455902\n",
      "Midterm Score: 75.600000  Predicted Score : 69.246909\n",
      "Midterm Score: 65.400000  Predicted Score : 63.873787\n",
      "Midterm Score: 52.600000  Predicted Score : 57.157384\n",
      "Midterm Score: 43.600000  Predicted Score : 52.455902\n",
      "Midterm Score: 61.500000  Predicted Score : 61.858866\n",
      "Midterm Score: 42.300000  Predicted Score : 51.784262\n",
      "Midterm Score: 60.300000  Predicted Score : 61.187226\n",
      "Midterm Score: 100.000000  Predicted Score : 82.008074\n",
      "Midterm Score: 70.500000  Predicted Score : 66.560348\n",
      "Midterm Score: 61.500000  Predicted Score : 61.858866\n",
      "Midterm Score: 0.000000  Predicted Score : 29.620134\n",
      "Midterm Score: 53.800000  Predicted Score : 57.829025\n",
      "Midterm Score: 34.600000  Predicted Score : 47.754421\n",
      "Midterm Score: 83.300000  Predicted Score : 73.276751\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 91.000000  Predicted Score : 77.306592\n",
      "Midterm Score: 29.500000  Predicted Score : 45.067860\n",
      "Midterm Score: 73.100000  Predicted Score : 67.903628\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 2.600000  Predicted Score : 30.963414\n",
      "Midterm Score: 61.500000  Predicted Score : 61.858866\n",
      "Midterm Score: 69.200000  Predicted Score : 65.888708\n",
      "Midterm Score: 48.700000  Predicted Score : 55.142463\n",
      "Midterm Score: 98.700000  Predicted Score : 81.336434\n",
      "Midterm Score: 34.600000  Predicted Score : 47.754421\n",
      "Midterm Score: 60.300000  Predicted Score : 61.187226\n",
      "Midterm Score: 44.900000  Predicted Score : 53.127543\n",
      "Midterm Score: 50.000000  Predicted Score : 55.814104\n",
      "Midterm Score: 69.200000  Predicted Score : 65.888708\n",
      "Midterm Score: 60.300000  Predicted Score : 61.187226\n",
      "Midterm Score: 83.300000  Predicted Score : 73.276751\n",
      "Midterm Score: 26.900000  Predicted Score : 43.724579\n",
      "Midterm Score: 61.500000  Predicted Score : 61.858866\n",
      "Midterm Score: 20.500000  Predicted Score : 40.366378\n",
      "Midterm Score: 19.200000  Predicted Score : 39.694737\n",
      "Midterm Score: 6.400000  Predicted Score : 32.978335\n",
      "Midterm Score: 80.800000  Predicted Score : 71.933470\n",
      "Midterm Score: 44.900000  Predicted Score : 53.127543\n",
      "Midterm Score: 26.900000  Predicted Score : 43.724579\n",
      "Midterm Score: 14.100000  Predicted Score : 37.008176\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 51.300000  Predicted Score : 56.485744\n",
      "Midterm Score: 24.400000  Predicted Score : 42.381298\n",
      "Midterm Score: 78.200000  Predicted Score : 70.590189\n",
      "Midterm Score: 23.100000  Predicted Score : 41.709658\n",
      "Midterm Score: 60.300000  Predicted Score : 61.187226\n",
      "Midterm Score: 71.800000  Predicted Score : 67.231988\n",
      "Midterm Score: 85.900000  Predicted Score : 74.620031\n",
      "Midterm Score: 50.000000  Predicted Score : 55.814104\n",
      "Midterm Score: 37.200000  Predicted Score : 49.097701\n",
      "Midterm Score: 73.100000  Predicted Score : 67.903628\n",
      "Midterm Score: 62.800000  Predicted Score : 62.530506\n",
      "Midterm Score: 59.000000  Predicted Score : 60.515586\n",
      "Midterm Score: 70.500000  Predicted Score : 66.560348\n",
      "Midterm Score: 5.100000  Predicted Score : 32.306695\n",
      "Midterm Score: 34.600000  Predicted Score : 47.754421\n",
      "Midterm Score: 19.200000  Predicted Score : 39.694737\n",
      "Midterm Score: 50.000000  Predicted Score : 55.814104\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 7.700000  Predicted Score : 33.649975\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 67.900000  Predicted Score : 65.217067\n",
      "Midterm Score: 53.800000  Predicted Score : 57.829025\n",
      "Midterm Score: 39.700000  Predicted Score : 50.440982\n",
      "Midterm Score: 43.600000  Predicted Score : 52.455902\n",
      "Midterm Score: 98.700000  Predicted Score : 81.336434\n",
      "Midterm Score: 79.500000  Predicted Score : 71.261830\n",
      "Midterm Score: 88.500000  Predicted Score : 75.963312\n",
      "Midterm Score: 55.100000  Predicted Score : 58.500665\n",
      "Midterm Score: 65.400000  Predicted Score : 63.873787\n",
      "Midterm Score: 57.700000  Predicted Score : 59.843945\n",
      "Midterm Score: 46.200000  Predicted Score : 53.799183\n",
      "Midterm Score: 51.900000  Predicted Score : 56.821564\n",
      "Midterm Score: 70.500000  Predicted Score : 66.560348\n",
      "Midterm Score: 51.300000  Predicted Score : 56.485744\n",
      "Midterm Score: 43.600000  Predicted Score : 52.455902\n",
      "Midterm Score: 6.400000  Predicted Score : 32.978335\n",
      "Midterm Score: 30.800000  Predicted Score : 45.739500\n",
      "Midterm Score: 96.200000  Predicted Score : 79.993153\n",
      "Midterm Score: 69.200000  Predicted Score : 65.888708\n",
      "Midterm Score: 64.100000  Predicted Score : 63.202147\n",
      "Midterm Score: 38.500000  Predicted Score : 49.769341\n",
      "Midterm Score: 42.300000  Predicted Score : 51.784262\n",
      "Midterm Score: 83.300000  Predicted Score : 73.276751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## BEGIN SOLUTION\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(mid)\n",
    "\n",
    "for i in range(len(mid)):\n",
    "    midterm = np.around(scaler.transform(mid)*100,decimals=1)[i][0]\n",
    "    predicted_final = np.around((theta0+ theta1*scaler.transform(mid))*100,decimals=10)[i]\n",
    "    string = 'Midterm Score: %f  Predicted Score : %f' % (midterm, predicted_final)    \n",
    "    print (string)\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-10, 10-highest) : 8\n",
    "* how can we improve his lab? : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_lab6.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; lab6 <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Developed by A.D. Gunawardena @ 2019 for CS 439"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
